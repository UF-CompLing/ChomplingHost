{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to ~nltk_data...\n",
      "[nltk_data]   Unzipping corpora/subjectivity.zip.\n",
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.835\n",
      "F-measure [obj]: 0.826315789474\n",
      "F-measure [subj]: 0.842857142857\n",
      "Precision [obj]: 0.872222222222\n",
      "Precision [subj]: 0.804545454545\n",
      "Recall [obj]: 0.785\n",
      "Recall [subj]: 0.885\n"
     ]
    }
   ],
   "source": [
    "# Title: Sentiment Analysis Example 1\n",
    "# Author: Dax Gerts\n",
    "# Date: 2 February 2016\n",
    "# Description: introductory example to sentiment analysis with python-nltk, based heavily on example provided by at www.nltk.org/howto/sentiment.html\n",
    "#   Example uses the 'sentiment anlyzer' tool to prepare data for classification with Naive Bayes Classifier\n",
    "\n",
    "## Modules\n",
    "\n",
    "# 1. NaiveBayesClassifier (machine learning method)\n",
    "\n",
    "# Naive Bayes Classifier is the machine learning model used in this example\n",
    "# More info https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# 2. Subjectivity (the data)\n",
    "\n",
    "# Corpus of phrases divided between subjective/objective\n",
    "\n",
    "# \"The Subjectivity Dataset contains 5000 subjective and 5000 objective processed sentences.\"\n",
    "# More info www.nltk.org/howto/corpus.html\n",
    "\n",
    "# For use in github notebook must manually download corpora\n",
    "nltk_dir = \"~nltk_data\"\n",
    "if nltk_dir not in nltk.data.path:\n",
    "        nltk.data.path.insert(0,nltk_dir)\n",
    "nltk.download(\"subjectivity\",download_dir=nltk_dir)\n",
    "#\n",
    "\n",
    "from nltk.corpus import subjectivity\n",
    "\n",
    "# 3. NLTK Sentiment Tools\n",
    "\n",
    "# The NLTK's collection of sentiment analysis tools\n",
    "# More info www.nltk.org\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "## Build training and testing data sets\n",
    "\n",
    "# Size of dataset(s)\n",
    "\n",
    "n = 1000\n",
    "\n",
    "# Get \"n\" subjective and objective phrases from subjectivity corpus\n",
    "\n",
    "subjective = [(sentences,'subj') for sentences in subjectivity.sents(categories='subj')[:n]]\n",
    "objective = [(sentences,'obj') for sentences in subjectivity.sents(categories='obj')[:n]]\n",
    "\n",
    "# Here's what the first item in \"subjective\" looks like\n",
    "# Note that it's stores as (phrase, label)\n",
    "\n",
    "subjective[0]\n",
    "\n",
    "# Create separate training and test data sets, this is pretty standard in any data mining/machine learning task\n",
    "# The typical split is, as seen here (training = 80%, train = 20%)\n",
    "\n",
    "training_subjective = subjective[:int(.8*n)]\n",
    "test_subjective = subjective[int(.8*n):n]\n",
    "training_objective = objective[:int(.8*n)]\n",
    "test_objective = objective[int(.8*n):n]\n",
    "\n",
    "# Now aggregate the training and test sets\n",
    "\n",
    "training = training_subjective + training_objective\n",
    "test = test_subjective + test_objective\n",
    "\n",
    "## Apply sentiment analysis to data to extract new \"features\"\n",
    "\n",
    "# Initialize sentiment analyzer object\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Mark all negative words in training data, using existing list of negative words\n",
    "\n",
    "all_negative_words = sentiment_analyzer.all_words([mark_negation(data) for data in training])\n",
    "\n",
    "unigram_features = sentiment_analyzer.unigram_word_feats(all_negative_words, min_freq=4)\n",
    "len(unigram_features)\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats,unigrams=unigram_features)\n",
    "\n",
    "training_final = sentiment_analyzer.apply_features(training)\n",
    "test_final = sentiment_analyzer.apply_features(test)\n",
    "\n",
    "## Traing model and test\n",
    "\n",
    "model = NaiveBayesClassifier.train\n",
    "classifer = sentiment_analyzer.train(model, training_final)\n",
    "\n",
    "for key, value in sorted(sentiment_analyzer.evaluate(test_final).items()):\n",
    "    print('{0}: {1}'.format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
